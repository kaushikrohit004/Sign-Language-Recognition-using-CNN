{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from PIL import Image, ImageDraw\n",
    "import imutils\n",
    "import tensorflow\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624, 100, 100, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = r\"D:\\Sign Language Recognization using CNN\\New folderr\\Gestures\"  # Gestures Images at Local machine\n",
    "\n",
    "num_classes=24        \n",
    "IMG_SIZE = 100\n",
    "def vectorize_data(TRAIN_DIR):\n",
    "    result = []\n",
    "    labels = []\n",
    "    for label in os.listdir(TRAIN_DIR):\n",
    "        path=\"\"\n",
    "        path=os.path.join(TRAIN_DIR, label)\n",
    "        for img in os.listdir(path):\n",
    "            path2=\"\"\n",
    "            path2 = os.path.join(path, img)\n",
    "            i = cv2.imread(path2)\n",
    "            \n",
    "            i = cv2.resize(cv2.imread(path2, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            \n",
    "            result.append(i)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return result, labels\n",
    "\n",
    "x, y =vectorize_data(TRAIN_DIR)\n",
    "x_train = np.array(x)\n",
    "y_train = np.array(y)\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "dictonary = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'K':9, 'L':10, 'M':11, 'N':12, 'O':13, 'P':14, \n",
    "            'Q':15, 'R':16, 'S':17, 'T':18, 'U':19, 'V':20, 'W':21, 'X':22, 'Y':23}\n",
    "num_classes=24\n",
    "keys, inv = np.unique(y_train, return_inverse=True)\n",
    "vals = np.array([dictonary[key] for key in keys])\n",
    "y_train_new = vals[inv]\n",
    "y_train_new_cat = to_categorical(y_train_new, num_classes)\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "x_new,y_new = unison_shuffled_copies(x_train,y_train_new_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "IMG_SIZE = 100\n",
    "\n",
    "num_classes = 24\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (2,2), input_shape=(IMG_SIZE, IMG_SIZE, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "281/281 [==============================] - 7s 23ms/step - loss: 5.4728 - accuracy: 0.0998 - val_loss: 2.8900 - val_accuracy: 0.2063\n",
      "Epoch 2/30\n",
      "281/281 [==============================] - 6s 23ms/step - loss: 2.7110 - accuracy: 0.1996 - val_loss: 2.7602 - val_accuracy: 0.2063\n",
      "Epoch 3/30\n",
      "281/281 [==============================] - 11s 39ms/step - loss: 2.3208 - accuracy: 0.2834 - val_loss: 2.7589 - val_accuracy: 0.1905\n",
      "Epoch 4/30\n",
      "281/281 [==============================] - 7s 24ms/step - loss: 2.0174 - accuracy: 0.3868 - val_loss: 2.2694 - val_accuracy: 0.4127\n",
      "Epoch 5/30\n",
      "281/281 [==============================] - 9s 32ms/step - loss: 1.7831 - accuracy: 0.4813 - val_loss: 2.1418 - val_accuracy: 0.4286\n",
      "Epoch 6/30\n",
      "281/281 [==============================] - 7s 24ms/step - loss: 1.5223 - accuracy: 0.5544 - val_loss: 2.0946 - val_accuracy: 0.4286\n",
      "Epoch 7/30\n",
      "281/281 [==============================] - 8s 30ms/step - loss: 1.2598 - accuracy: 0.6185 - val_loss: 2.1839 - val_accuracy: 0.4444\n",
      "Epoch 8/30\n",
      "281/281 [==============================] - 13s 45ms/step - loss: 1.1256 - accuracy: 0.6578 - val_loss: 1.9853 - val_accuracy: 0.4762\n",
      "Epoch 9/30\n",
      "281/281 [==============================] - 12s 44ms/step - loss: 0.9517 - accuracy: 0.7380 - val_loss: 1.6533 - val_accuracy: 0.5556\n",
      "Epoch 10/30\n",
      "281/281 [==============================] - 13s 45ms/step - loss: 0.8654 - accuracy: 0.7362 - val_loss: 1.5807 - val_accuracy: 0.5714\n",
      "Epoch 11/30\n",
      "281/281 [==============================] - 8s 30ms/step - loss: 0.7690 - accuracy: 0.7433 - val_loss: 1.6471 - val_accuracy: 0.5714\n",
      "Epoch 12/30\n",
      "281/281 [==============================] - 7s 23ms/step - loss: 0.6362 - accuracy: 0.7879 - val_loss: 1.4720 - val_accuracy: 0.6032\n",
      "Epoch 13/30\n",
      "281/281 [==============================] - 7s 26ms/step - loss: 0.5868 - accuracy: 0.8021 - val_loss: 1.3590 - val_accuracy: 0.6667\n",
      "Epoch 14/30\n",
      "281/281 [==============================] - 7s 25ms/step - loss: 0.4644 - accuracy: 0.8396 - val_loss: 1.4190 - val_accuracy: 0.6349\n",
      "Epoch 15/30\n",
      "281/281 [==============================] - 7s 24ms/step - loss: 0.4554 - accuracy: 0.8627 - val_loss: 1.4122 - val_accuracy: 0.6349\n",
      "Epoch 16/30\n",
      "281/281 [==============================] - 7s 25ms/step - loss: 0.3875 - accuracy: 0.8806 - val_loss: 1.4280 - val_accuracy: 0.6508\n",
      "Epoch 17/30\n",
      "281/281 [==============================] - 7s 24ms/step - loss: 0.3978 - accuracy: 0.8645 - val_loss: 1.3486 - val_accuracy: 0.6667\n",
      "Epoch 18/30\n",
      "281/281 [==============================] - 8s 28ms/step - loss: 0.3576 - accuracy: 0.8788 - val_loss: 1.2006 - val_accuracy: 0.7143\n",
      "Epoch 19/30\n",
      "281/281 [==============================] - 8s 29ms/step - loss: 0.2873 - accuracy: 0.9180 - val_loss: 1.2528 - val_accuracy: 0.6508\n",
      "Epoch 20/30\n",
      "281/281 [==============================] - 7s 26ms/step - loss: 0.2877 - accuracy: 0.9091 - val_loss: 1.0952 - val_accuracy: 0.7143\n",
      "Epoch 21/30\n",
      "281/281 [==============================] - 11s 39ms/step - loss: 0.2387 - accuracy: 0.9323 - val_loss: 1.2606 - val_accuracy: 0.7460\n",
      "Epoch 22/30\n",
      "281/281 [==============================] - 11s 39ms/step - loss: 0.2129 - accuracy: 0.9269 - val_loss: 1.1628 - val_accuracy: 0.6825\n",
      "Epoch 23/30\n",
      "281/281 [==============================] - 11s 39ms/step - loss: 0.2005 - accuracy: 0.9519 - val_loss: 1.0050 - val_accuracy: 0.7143\n",
      "Epoch 24/30\n",
      "281/281 [==============================] - 11s 40ms/step - loss: 0.2587 - accuracy: 0.9269 - val_loss: 1.1428 - val_accuracy: 0.7302\n",
      "Epoch 25/30\n",
      "281/281 [==============================] - 6s 22ms/step - loss: 0.1574 - accuracy: 0.9590 - val_loss: 1.0653 - val_accuracy: 0.7143\n",
      "Epoch 26/30\n",
      "281/281 [==============================] - 12s 44ms/step - loss: 0.1792 - accuracy: 0.9465 - val_loss: 1.0827 - val_accuracy: 0.7143\n",
      "Epoch 27/30\n",
      "281/281 [==============================] - 12s 44ms/step - loss: 0.1249 - accuracy: 0.9697 - val_loss: 1.2258 - val_accuracy: 0.7143\n",
      "Epoch 28/30\n",
      "281/281 [==============================] - 12s 44ms/step - loss: 0.1628 - accuracy: 0.9519 - val_loss: 1.1552 - val_accuracy: 0.7302\n",
      "Epoch 29/30\n",
      "281/281 [==============================] - 10s 37ms/step - loss: 0.1513 - accuracy: 0.9572 - val_loss: 1.0495 - val_accuracy: 0.7302\n",
      "Epoch 30/30\n",
      "281/281 [==============================] - 11s 40ms/step - loss: 0.1203 - accuracy: 0.9643 - val_loss: 0.9550 - val_accuracy: 0.7302\n",
      "INFO:tensorflow:Assets written to: Rohit_1.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_new, y_new, epochs = 30, validation_split = 0.1, shuffle = True, batch_size = 2)\n",
    "model.save(\"Rohit_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "model = load_model(\"Rohit_1.model\")\n",
    "IMG_SIZE = 100\n",
    "top, right, bottom, left = 10, 350, 225, 590\n",
    "_,first_frame = camera.read()\n",
    "SAVE_PATH = \"C:/Users/Admin/Downloads/Compressed\"\n",
    "first_gray = cv2.flip(first_frame, 1)\n",
    "roi = first_gray[top:bottom, right:left]\n",
    "img_count = 1\n",
    "dictonary = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'K':9, 'L':10, 'M':11, 'N':12, 'O':13, 'P':14, \n",
    "            'Q':15, 'R':16, 'S':17, 'T':18, 'U':19, 'V':20, 'W':21, 'X':22, 'Y':23}\n",
    "num_class = 24\n",
    "out = \"\"\n",
    "inv_dictonary = dict(map(reversed, dictonary.items()))\n",
    "string = \"\"\n",
    "\n",
    "while True:\n",
    "    \n",
    "    _, frame = camera.read()\n",
    "    \n",
    "    \n",
    "    gray_frame = cv2.flip(frame, 1) \n",
    "    \n",
    "    \n",
    "    roi2 = gray_frame[top:bottom, right:left] \n",
    "    cv2.rectangle(gray_frame, (300, 70), (600, 250), (0,255,0), 2) \n",
    "    \n",
    "    diff = cv2.absdiff(roi, roi2)\n",
    "    diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    diff = cv2.GaussianBlur(diff, (11,11),0)\n",
    "    _,diff = cv2.threshold(diff, 20, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    cv2.imshow(\"Diff\", diff)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    if key == ord('r'):\n",
    "        roi = gray_frame[top:bottom, right:left]\n",
    "        string = \"\"\n",
    "        \n",
    "    img = cv2.resize(diff, (IMG_SIZE, IMG_SIZE))\n",
    "    img = tensorflow.cast(img, tensorflow.float32)\n",
    "\n",
    "    img_arr = np.array(img)\n",
    "\n",
    "    t2 = np.expand_dims(img_arr, axis=-1)\n",
    "    t2 = np.expand_dims(t2, axis=0)\n",
    "    p = model.predict(t2)\n",
    "    p2 = np.argmax(p)\n",
    "    out = inv_dictonary[p2]\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(gray_frame,out, (22,34), font, 1, (200,255,255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(gray_frame,string, (22,64), font, 1, (200,255,255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(gray_frame,\"r: Reset;  s: Append;  q: Quit\", (22,470), font, 1, (200,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow(\"Frame\", gray_frame) #gray_frame\n",
    "    if key == ord('s'):\n",
    "        string = string+\"\"+out\n",
    "        \n",
    "    if key == ord('c'):\n",
    "        img_name = \"data{}.png\".format(img_count)\n",
    "        img_count+=1\n",
    "        cv2.imwrite(os.path.join(SAVE_PATH, img_name), diff)\n",
    "        \n",
    "        img_name = \"data{}.png\".format(img_count)\n",
    "        img_count+=1\n",
    "        cv2.imwrite(os.path.join(SAVE_PATH, img_name), gray_frame)\n",
    "        \n",
    "        img_name = \"data{}.png\".format(img_count)\n",
    "        img_count+=1\n",
    "        cv2.imwrite(os.path.join(SAVE_PATH, img_name), roi2)\n",
    "   \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
